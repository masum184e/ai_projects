{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a6e042-083a-4795-a41c-2e32acd093a4",
   "metadata": {},
   "source": [
    "# Coffeshop Chatbot\n",
    "It focuses on training a `neural network model` to classify user input into predefined intents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc6744-666b-49ed-87b6-7fe4ce632281",
   "metadata": {},
   "source": [
    "# Imports and NLTK Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45b61c9-f976-42fd-b7ec-b0a9d14494bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668dc41-e59a-4685-8ff1-94149f211741",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fb7ae8-6c37-4c77-854b-e09b6b9b6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757b5ed-3bf6-4b82-b329-69acd884a7ba",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6da4e13-d51b-45bd-bdd8-2e9e8680168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/coffeshop_chat.json\", 'r') as f:\n",
    "    intents=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038c254-0ae2-4abe-8ffd-3353da0ac217",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b71360-061c-478e-8f6d-e82df3c62c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2258f158-c89c-4b28-b8cc-b5728871c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "tags=[]\n",
    "tokens_tag=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ccf345-347e-40dc-b841-63a9c7d6193f",
   "metadata": {},
   "source": [
    "- `words` - Stores all the words (tokens) from the patterns in the intents.\n",
    "- `tags` - Stores the different tags (categories) from the intents.\n",
    "- `tokens_tag` is a list of tuples, each containing a list of words (tokens) and the corresponding tag.\n",
    "__Sample:__\n",
    "```\n",
    " (['Thank', \"'s\", 'a', 'lot', '!'], 'thanks'),\n",
    " (['Which', 'items', 'do', 'you', 'have', '?'], 'items'),\n",
    " (['What', 'kinds', 'of', 'items', 'are', 'there', '?'], 'items'),\n",
    " (['What', 'do', 'you', 'sell', '?'], 'items'),\n",
    " (['Do', 'you', 'take', 'credit', 'cards', '?'], 'payments'),\n",
    "```\n",
    "`Thank's a lot!` converted to `['Thank', \"'s\", 'a', 'lot', '!']` and the corresponding tag is `thanks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35829e2-4089-4ea7-a70f-96685ca73532",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    tag=intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        word=word_tokenize(pattern)\n",
    "        words.extend(word)\n",
    "\n",
    "        tokens_tag.append((word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ace9c-77bd-453a-828d-001a6d757d3b",
   "metadata": {},
   "source": [
    "## append()\n",
    "__Add a Single Element to the End of a List__\n",
    "```python\n",
    "my_list = [1, 2, 3]\n",
    "my_list.append(4)\n",
    "print(my_list)\n",
    "# Output: [1, 2, 3, 4]\n",
    "```\n",
    "It keep the new item as it is.\n",
    "```python\n",
    "# Appending another list as a single item\n",
    "my_list.append([5, 6])\n",
    "print(my_list)\n",
    "# Output: [1, 2, 3, 4, [5, 6]]  # The entire list is added as one element\n",
    "```\n",
    "## extend()\n",
    "__Add Multiple Elements (from an Iterable) to a List__\n",
    "```python\n",
    "my_list = [1, 2, 3]\n",
    "my_list.extend([4, 5, 6])\n",
    "print(my_list)\n",
    "# Output: [1, 2, 3, 4, 5, 6]  # Each element of [4, 5, 6] is added individually\n",
    "```\n",
    "It convert the new item into iterable item\n",
    "```python\n",
    "# Extending with a string\n",
    "my_list.extend(\"abc\")\n",
    "print(my_list)\n",
    "# Output: [1, 2, 3, 4, 5, 6, 'a', 'b', 'c']\n",
    "```\n",
    "## enumerate()\n",
    "__Iterate Over a List (or Iterable) and Get Index Along With Element__\n",
    "```python\n",
    "my_list = ['apple', 'banana', 'cherry']\n",
    "\n",
    "for idx, fruit in enumerate(my_list):\n",
    "    print(idx, fruit)\n",
    "```\n",
    "You can also specify a different starting index:\n",
    "```python\n",
    "for idx, fruit in enumerate(my_list, start=1):\n",
    "    print(idx, fruit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157164e-8fbd-4afd-a6bd-fd4f2066db0f",
   "metadata": {},
   "source": [
    "# Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84f79542-2efe-41a6-894a-298393bf6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words=['?','!','.',',','\\'s']\n",
    "words=[stemmer.stem(word) for word in words if word not in ignore_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47174f34-804c-4ebf-a5e0-a476ba736a9d",
   "metadata": {},
   "source": [
    "## set()\n",
    "It covert the list into a set, which automatically removes duplicate words. The model would process duplicate words, increasing memory usage and possibly affecting performance.\n",
    "## sorted()\n",
    "It is used to order the words and tags alphabetically or numerically (depending on the content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e27cac9-4b08-4305-ac22-14f695d7fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=sorted(set(words))\n",
    "tags=sorted(set(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccce48f-9c0e-40ac-a815-be33e1e18977",
   "metadata": {},
   "source": [
    "# Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03cab9f9-95ab-4448-a6e3-7d58470a672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokens, words):\n",
    "    stemmed_token=[stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    bag=np.zeros(len(words), dtype=np.float32)\n",
    "\n",
    "    for idx, word in enumerate(words):\n",
    "        if word in stemmed_token:\n",
    "            bag[idx]=1.0\n",
    "        \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f84d3d-09f2-4298-9c9e-521d764949e0",
   "metadata": {},
   "source": [
    "- `tokens` - tokens of a specifc pattern\n",
    "- `words` - all the words (tokens) from the patterns in the intents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd4015-cfb8-43e1-9739-4574a9307f5d",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51ca1da7-68e1-47b5-a229-a4d487b6ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "patterns = [' '.join(tokens) for tokens, tag in tokens_tag]\n",
    "vectorizer.fit(patterns)\n",
    "\n",
    "X_train = vectorizer.transform(patterns).toarray()\n",
    "y_train = [tags.index(tag) for tokens, tag in tokens_tag]\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8c0f1-b966-4064-88f4-375188052fcd",
   "metadata": {},
   "source": [
    "## Custom BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1918e3f4-b241-4457-854a-8f01c3668ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=[]\n",
    "# y_train=[]\n",
    "# for (tokens, tag) in tokens_tag:\n",
    "#     bow=bag_of_words(tokens, words)\n",
    "#     X_train.append(bow)\n",
    "\n",
    "#     y_train.append(tags.index(tag))\n",
    "\n",
    "# X_train=np.array(X_train)\n",
    "# y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a09389-ecf3-4e70-827d-0f4537b0008f",
   "metadata": {},
   "source": [
    "# Building the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdbc6a-5e31-46d0-8112-540c03b9cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(tags), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=8, verbose=1)\n",
    "model.save(\"chatbot_model.keras\")\n",
    "\n",
    "with open('words.pkl', 'wb') as f:\n",
    "    pickle.dump(words, f)\n",
    "\n",
    "with open('tags.pkl', 'wb') as f:\n",
    "    pickle.dump(tags, f)\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496355d-ce69-4f41-93b2-a89ab1e8c6a7",
   "metadata": {},
   "source": [
    "# Files\n",
    "`.keras` and `.pkl` files are used to save trained models and associated data structures so that they can be reused later without needing to retrain the model or recreate the necessary data processing structures.\n",
    "## `.keras`\n",
    "It is used to save the entire trained model. This includes:\n",
    "- The model architecture (layers, neurons, activation functions, etc.).\n",
    "- The model weights (the learned parameters from training).\n",
    "- The optimizer configuration (so the model can be reloaded and continued training if needed).\n",
    "## `.pkl`\n",
    "It is used to save Python objects to a file, which can be later loaded into memory.\n",
    "- `words.pkl` - Contains the list of all unique words (tokens) that the model was trained on.\n",
    "- `tags.pkl` - Contains the list of unique tags (intents) that the model can classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcefdca-c60a-4fc9-a2be-ee40cd69cde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
